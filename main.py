import argparse
import json
from pathlib import Path
import pandas as pd
from datetime import datetime

from utils.config_loader import load_config 
from data.loader import load_raw_dataset
from model.model_loader import load_model
from model.model_runner import model_inference # ğŸ†• ì¶”ë¡  ì—”ì§„ í•¨ìˆ˜ë¥¼ ì§ì ‘ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from metrics.auroc import calculate_auroc_scores
from metrics.classification import calculate_classification_metrics



def main(config_path: str, save_dir_base: str | None):
    """
    ì£¼ì–´ì§„ ì„¤ì • íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    settings = load_config(config_path)
    print(f"===== Starting Pipeline with config: {config_path} =====")

    # 1. ì €ì¥ ê²½ë¡œ ì„¤ì • (ì˜µì…˜ ìš°ì„ )
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_name = f"{settings.model.name}_{timestamp}"
    experiment_dir = Path(save_dir_base) / experiment_name

    output_dir = experiment_dir / "model_outputs"
    metrics_dir = experiment_dir / "metrics"

    output_dir.mkdir(parents=True, exist_ok=True)
    metrics_dir.mkdir(parents=True, exist_ok=True)

    # 2. ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ 1íšŒ)
    pipe, tokenizer = load_model(settings.model.name, settings.model.path)

    # 3. ë°ì´í„°ì…‹ë³„ ì¶”ë¡  ë° ê²°ê³¼ ì €ì¥
    for dataset_name, dataset_config in settings.datasets.items():
        try:
            print(f"\n--- Processing Dataset: {dataset_name} (type: {dataset_config.type}) ---")
            raw_data = load_raw_dataset(dataset_name, settings.sample_num)
            
            # ì¶”ë¡  ì‹¤í–‰
            results = model_inference(
                method=settings.model.inference_method,
                params=settings.model.inference_params,
                pipe=pipe,
                tokenizer=tokenizer,
                data=raw_data,
                dataset_type=dataset_config.type
            )
            #ê²°ê³¼ ì €ì¥
            output_path = output_dir / f"{dataset_name}_output.json"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=4, ensure_ascii=False)
            print(f"Results for '{dataset_name}' saved to: {output_path}")

        except Exception as e:
            print(f"!!! Error processing {dataset_name}: {e}")
            continue
    
    # 4. ëª¨ë¸ í‰ê°€
    print("\n--- Evaluating Model Performance ---")
    final_summary = {}
    # ê°ì²´ì˜ ì†ì„±ì— ì§ì ‘ ì ‘ê·¼
    SCORE_COLUMNS = settings.model.inference_params.scorers
    
    for dataset_name in settings.datasets_to_evaluate:
        results_path = output_dir / f"{dataset_name}_output.json"
        
        if not results_path.exists():
            print(f"Warning: Results file not found for '{dataset_name}'. Skipping evaluation.")
            continue
        
        results_df = pd.read_json(results_path)
        # 'prediction'ê³¼ 'ground_truth'ê°€ ëª¨ë‘ ë¬¸ìì—´ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ íƒ€ì… ë³€í™˜
        results_df['response_correct'] = results_df['prediction'].astype(str) == results_df['ground_truth'].astype(str)
        
        print(f"\n--- Metrics for {dataset_name} ---")
        auroc_results = calculate_auroc_scores(results_df, SCORE_COLUMNS)
        print("AUROC Scores:", auroc_results)

        class_results = calculate_classification_metrics(results_df, SCORE_COLUMNS)
        print("Classification Metrics:", class_results)


        final_summary[dataset_name] = {
            "auroc_scores": auroc_results, # '.to_dict(...)' ì œê±°
            "classification_metrics": class_results
        }

    # 5. ìµœì¢… í‰ê°€ ê²°ê³¼ ì €ì¥
    metrics_path = metrics_dir / "summary.json"
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(final_summary, f, indent=4)
        
    print(f"\nFinal metrics summary saved to: {metrics_path}")
    print("\n===== Pipeline Complete =====")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run the Hallucination Detection Pipeline.")
    parser.add_argument('-c', '--config', type=str, default='configs/config.yaml', help='Path to the configuration YAML file.')
    parser.add_argument('--save_dir', type=str, default='results', help='Directory to save all results, overriding config file settings.')
    args = parser.parse_args()
    main(config_path=args.config, save_dir_base=args.save_dir)

#python main.py -c configs/config.yaml --save_dir ./results