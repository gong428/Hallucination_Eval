
import torch
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
from uqlm.scorers import WhiteBoxUQ
from typing import List, Dict


def _extract_logprobs_detailed(outputs, input_length: int, tokenizer) -> tuple[List[Dict[str, float]], Dict]:
    """
    generate()ì˜ ì¶œë ¥ì—ì„œ 'ìƒì„±ëœ ë‹µë³€' êµ¬ê°„ì˜ í† í°ë³„ ë¡œê·¸í™•ë¥ ì„ ì¶”ì¶œí•´
    (1) WhiteBoxUQì— ë§ëŠ” ìµœì†Œ í¬ë§·, (2) ì‚¬ëŒì´ ë³´ê¸° ì¢‹ì€ ìƒì„¸ í¬ë§·ì„ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns
    -------
    tuple:
        - wbuq_logprobs: List[{'logprob': float}]
            WhiteBoxUQ.score(logprobs_results=...)ì— ë°”ë¡œ ë„£ì„ ìˆ˜ ìˆëŠ” ìµœì†Œ í˜•ì‹
        - detail_summary: Dict
            ì‚¬ëŒì´ ë³´ê¸° ì¢‹ì€ ìƒì„¸ ì •ë³´:
            {
              'tokens': [
                  {'idx': int, 'id': int, 'text': str, 'logprob': float, 'prob': float}
              ],
              'sum_logprob': float,
              'avg_logprob': float,
              'geom_mean_prob': float,
              'min_logprob': float,
              'min_token': {'idx':..., 'id':..., 'text':..., 'logprob':..., 'prob':...}
            }
    """
    # 1) ìƒì„±ëœ ì‹œí€€ìŠ¤ë§Œ ë¶„ë¦¬
    generated_sequence = outputs.sequences[0][input_length:]

    # 2) ê° stepì˜ ë¡œì§“ -> ë¡œê·¸í™•ë¥ 
    logprobs_list = [torch.nn.functional.log_softmax(score, dim=-1) for score in outputs.scores]

    # 3) EOS í† í° ì œê±°
    seq_to_process = generated_sequence
    if len(seq_to_process) > 0 and tokenizer.eos_token_id is not None and seq_to_process[-1] == tokenizer.eos_token_id:
        seq_to_process = seq_to_process[:-1]

    tokens_detail = []
    wbuq_logprobs = []
    sum_lp = 0.0
    min_lp = float('inf')
    min_tok = None

    # 4) í† í°ë³„ ë¡œê·¸í™•ë¥  ìˆ˜ì§‘
    for i, token_id in enumerate(seq_to_process):
        # i-th ìƒì„± í† í°ì€ i-th scoreì™€ ë§¤ì¹­
        lp = logprobs_list[i][0, token_id].item()
        prob = float(np.exp(lp))
        text = tokenizer.decode([token_id], skip_special_tokens=True)

        # --- â¬‡ï¸ Top-10 ì¶”ì¶œ ë¡œì§ ì¶”ê°€ ---
        token_probs = torch.nn.functional.softmax(outputs.scores[i], dim=-1)[0]
        top10_probs, top10_indices = torch.topk(token_probs, 10)
        top10_tokens_text = tokenizer.convert_ids_to_tokens(top10_indices)
        top_10_info = [{'token': t, 'prob': p.item()} for t, p in zip(top10_tokens_text, top10_probs)]
        # --- â¬†ï¸ Top-10 ì¶”ì¶œ ë¡œì§ ë ---



        entry = {
            'idx': i, 
            'id': int(token_id), 
            'text': text, 
            'logprob': lp, 
            'prob': prob,
            'top_10_tokens': top_10_info  # ğŸ†• Top-10 ì •ë³´ ì¶”ê°€
        }
        tokens_detail.append(entry)
        wbuq_logprobs.append({'logprob': lp})

        sum_lp += lp
        if lp < min_lp:
            min_lp = lp
            min_tok = entry

    # 5) ìš”ì•½ í†µê³„
    n = max(len(tokens_detail), 1)
    avg_lp = sum_lp / n
    geom_mean_prob = float(np.exp(avg_lp))

    detail_summary = {
        'tokens': tokens_detail,
        'sum_logprob': sum_lp,
        'avg_logprob': avg_lp,
        'geom_mean_prob': geom_mean_prob,
        'min_logprob': min_lp if min_tok is not None else None,
        'min_token': min_tok,
    }
    return wbuq_logprobs, detail_summary

def _extract_logprobs(outputs, input_length: int, tokenizer) -> List[Dict[str, float]]:
    """
    Hugging Face ëª¨ë¸ì˜ generate ì¶œë ¥ì—ì„œ í† í°ë³„ logprobì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ê°€ ìƒì„±í•˜ëŠ” [{'logprob': ê°’}] í˜•ì‹ì€ WhiteBoxUQ í´ë˜ìŠ¤ì˜ 
    get_logprobs, _get_probs ë©”ì†Œë“œê°€ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ í˜•ì‹ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤.
    """
    # 1. ìƒì„±ëœ ì‹œí€€ìŠ¤(ë‹µë³€) ë¶€ë¶„ë§Œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    generated_sequence = outputs.sequences[0][input_length:]
    
    # 2. ëª¨ë¸ì˜ ì¶œë ¥ ì ìˆ˜(logits)ë¥¼ ë¡œê·¸ í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    logprobs_list = [torch.nn.functional.log_softmax(score, dim=-1) for score in outputs.scores]
    
    # --- Top-10 ì¶”ì¶œ ë¡œì§ ì‹œì‘ ---
    prompt_top10_data = []
    all_top10_probs = []

    for i, token_logits in enumerate(outputs.scores):
        # ë¡œì§“ì„ ì „ì²´ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜ (ì†Œí”„íŠ¸ë§¥ìŠ¤)
        token_probs = torch.nn.functional.softmax(token_logits, dim=-1)[0]
        
        # í™•ë¥ ì´ ê°€ì¥ ë†’ì€ 10ê°œì™€ ê·¸ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŒ
        top10_probs, top10_indices = torch.topk(token_probs, 10)
        
        # ì¸ë±ìŠ¤(í† í° ID)ë¥¼ ì‹¤ì œ í† í°(ë¬¸ì)ìœ¼ë¡œ ë³€í™˜
        top10_tokens = tokenizer.convert_ids_to_tokens(top10_indices)
        
        # í•´ë‹¹ ë‹¨ê³„ì˜ top-10 ì •ë³´ë¥¼ ì €ì¥
        step_top10 = [{'token': token, 'prob': prob.item()} for token, prob in zip(top10_tokens, top10_probs)]
        prompt_top10_data.append(step_top10)
    all_top10_probs.append(prompt_top10_data)
    # --- Top-10 ì¶”ì¶œ ë¡œì§ ë ---
    
    # --- â¬‡ï¸ EOS í† í° ì œì™¸ ë¡œì§ ì¶”ê°€ ---
    sequence_to_process = generated_sequence
    # ë§ˆì§€ë§‰ í† í°ì´ EOS í† í°ì´ë©´, ì²˜ë¦¬í•  ì‹œí€€ìŠ¤ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
    if len(sequence_to_process) > 0 and sequence_to_process[-1] == tokenizer.eos_token_id:
        sequence_to_process = sequence_to_process[:-1]
    # --- â¬†ï¸ EOS í† í° ì œì™¸ ë¡œì§ ë ---

    sequence_logprobs = []
    # 3. ìƒì„±ëœ ê° í† í°ì— í•´ë‹¹í•˜ëŠ” ë¡œê·¸ í™•ë¥  ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    # ì´ì œ EOSê°€ ì œì™¸ëœ ì‹œí€€ìŠ¤ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤.
    for i, token_id in enumerate(sequence_to_process):
        token_logprob = logprobs_list[i][0, token_id].item()
        sequence_logprobs.append({'logprob': token_logprob})
        
    return sequence_logprobs, all_top10_probs