import torch
from transformers import AutoTokenizer, pipeline
from typing import Optional, Dict, Any

def _get_torch_dtype(dtype_str: Optional[str] = None) -> Optional[torch.dtype]:
    """YAMLì— ëª…ì‹œëœ ë¬¸ìì—´ì„ ì‹¤ì œ torch.dtype ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if not dtype_str:
        return None
    
    # torch ê°ì²´ì—ì„œ í•´ë‹¹ ë°ì´í„° íƒ€ì… ì°¾ê¸°
    return getattr(torch, dtype_str, None)

def load_model(model_name: str, model_path: str, model_kwargs: Optional[Dict[str, Any]] = None):
    """
    ë¡œì»¬ ê²½ë¡œ ë˜ëŠ” Hugging Face Hubì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    config.yamlì˜ model_kwargs ì„¤ì •ì„ ë™ì ìœ¼ë¡œ ì ìš©í•©ë‹ˆë‹¤.

    Args:
        model_name (str): ëª¨ë¸ì˜ ì´ë¦„ (ë¡œê·¸ ì¶œë ¥ìš©).
        model_path (str): ëª¨ë¸ì˜ ë¡œì»¬ ê²½ë¡œ ë˜ëŠ” Hub ID.
        model_kwargs (dict, optional): ëª¨ë¸ ë¡œë”© ì‹œ ì‚¬ìš©í•  ì¶”ê°€ ì¸ì.

    Returns:
        tuple: ë¡œë“œëœ íŒŒì´í”„ë¼ì¸ ê°ì²´ì™€ í† í¬ë‚˜ì´ì €.
    """
    print(f"--- Loading Model: {model_name} from '{model_path}' ---")

    # model_kwargsê°€ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
    kwargs = model_kwargs if model_kwargs else {}

    # torch_dtype ë¬¸ìì—´ì„ ì‹¤ì œ torch.dtype ê°ì²´ë¡œ ë³€í™˜
    if 'torch_dtype' in kwargs and isinstance(kwargs['torch_dtype'], str):
        dtype = _get_torch_dtype(kwargs['torch_dtype'])
        if dtype:
            kwargs['torch_dtype'] = dtype
            print(f"Converted torch_dtype string to {dtype}")
        else:
            print(f"Warning: Could not find torch.dtype for '{kwargs['torch_dtype']}'. Ignoring.")
            del kwargs['torch_dtype']

    # bnb_4bit_compute_dtype ë³€í™˜
    if 'bnb_4bit_compute_dtype' in kwargs and isinstance(kwargs['bnb_4bit_compute_dtype'], str):
        dtype = _get_torch_dtype(kwargs['bnb_4bit_compute_dtype'])
        if dtype:
            kwargs['bnb_4bit_compute_dtype'] = dtype
            print(f"Converted bnb_4bit_compute_dtype string to {dtype}")
        else:
            print(f"Warning: Could not find torch.dtype for '{kwargs['bnb_4bit_compute_dtype']}'. Ignoring.")
            del kwargs['bnb_4bit_compute_dtype']

    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # text-generation íŒŒì´í”„ë¼ì¸ ìƒì„±
        pipe = pipeline(
            "text-generation",
            model=model_path,
            tokenizer=tokenizer,
            model_kwargs=kwargs,  # ğŸ†• ë™ì ìœ¼ë¡œ ìƒì„±ëœ kwargs ì „ë‹¬
            device_map="auto"
        )
        print("âœ… Model and Tokenizer loaded successfully with options:", kwargs)
        return pipe, tokenizer

    except Exception as e:
        print(f"âŒ Error loading model '{model_path}': {e}")
        print("ëª¨ë¸ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€, í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(ì˜ˆ: bitsandbytes, accelerate)ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        raise

def load_model_path(model_name: str, model_path: str):
    """
    ì§€ì •ëœ ê²½ë¡œì—ì„œ transformers íŒŒì´í”„ë¼ì¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    4bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.
    
    Returns:
        tuple: (pipe, tokenizer) ê°ì²´.
    """
    print(f"--- Loading model '{model_name}' from path: {model_path} ---")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ì„¤ì •
    pipe = pipeline(
        "text-generation",
        model=model_path,
        tokenizer=tokenizer,
        model_kwargs={
            "torch_dtype": torch.bfloat16,
            "load_in_4bit": True, # 4bit ì–‘ìí™” ì‚¬ìš©
        },
        device_map="auto" # ì‚¬ìš© ê°€ëŠ¥í•œ GPUì— ëª¨ë¸ ìë™ í• ë‹¹
    )
    
    print("Pipeline and tokenizer loaded successfully.")
    # â—ì£¼ì˜: ëª¨ë¸ ìì²´ ëŒ€ì‹  íŒŒì´í”„ë¼ì¸ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    return pipe, tokenizer